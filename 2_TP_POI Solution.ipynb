{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkFdE1m3Sl7w"
   },
   "source": [
    "#**Recommandation de points d'intérêts sur les réseaux sociaux**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS7Qv0nCSvk7"
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "L'objectif de ce TP est d'implémenter un algorithme de recommandation de points d'intérêt en se basant sur les données publiées sur les réseaux sociaux. Ces données couvrent en particulier les localisations des visites des utilisateurs ainsi que leurs relations sociales (ou \"amis\"), et peuvent être utilisées pour modéliser les préférences des utilisateurs et générer les recommandations. \n",
    "\n",
    "Ce document décrit les différentes étapes pour l'implémentation de l'approche *iGSLR* qui se base sur du filtrage collaboratif et sur l'exploitation des influences géographiques et sociales. \n",
    "\n",
    "*(Solution initialement présentée dans: \"iGSLR: Personalized Geo-Social Location Recommendation: A Kernel Density Estimation Approach\", Zhang and Chow, SIGSPATIAL'13)*\n",
    "\n",
    "L'implémentation du modèle peut se faire à l'aide de la libraire **Surprise** (http://surpriselib.com/).\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic \n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from surprise import AlgoBase\n",
    "from surprise import PredictionImpossible\n",
    "from itertools import combinations\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import BaselineOnly\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise.accuracy import rmse\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXspKeHGTVT0"
   },
   "source": [
    "# 2. Chargement et traitement des données\n",
    "\n",
    "Le jeu de données utilisé dans le cadre de ce TP a été collecté à partir du réseau social *Gowalla*. Il contient des informations concernant les profils des utilisateurs, leurs relations sociales et les localisations de leurs visites (qu'on associera à la dénomination \"POI\" dans la suite). \n",
    "\n",
    "Le jeu de données comprend 36,001,959 visites faites par 407,533 utilisateurs dans 2,724,891 POIs, et peut être téléchargé à partir de ce lien (https://drive.google.com/file/d/1DBq98KRWGRQyAJ0TqBqBKETGT90cyLwh/view?usp=sharing).\n",
    "\n",
    "Dans la suite, il sera possible à tout moment de travailler sur un échantillon tiré de ce jeu de données, afin de faciliter les calculs. \n",
    "\n",
    "Afin de traiter et charger les données, effectuer les étapes suivantes:\n",
    "*   Extraire le jeu de données et le charger dans un *dataframe* de *pandas*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checkins = pd.read_csv(\"./gowalla/gowalla_checkins.csv\")\n",
    "df_friendship = pd.read_csv(\"./gowalla/gowalla_friendship.csv\")\n",
    "df_locations = pd.read_csv(\"./gowalla/gowalla_locations.csv\")\n",
    "df_userinfo = pd.read_csv(\"./gowalla/gowalla_userinfo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only a small dataset for comptung reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_checkins = df_checkins.head(200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jJIGFwuad-b"
   },
   "source": [
    "*   Retirer les utilisateurs qui ont effectué moins de 5 visites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculer le nombre de checkins pour chaque utilisateur\n",
    "df_grouped_checkins  = df_checkins.groupby(['userid'] , as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recuperer seulement les uderid avec plus de 5 places visités et il faut réduire aussi à moins de 50 lieux à cause de la complexité\n",
    "filtered_user_ids = df_grouped_checkins[(df_grouped_checkins.placeid >= 5) & (df_grouped_checkins.placeid <= 50)].userid.values \n",
    "df_filtered_checkins = df_checkins[df_checkins.userid.isin(filtered_user_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer dtaframe df_friendship aussi\n",
    "df_filtered_freindship = df_friendship[df_friendship.userid1.isin(filtered_user_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer dtaframe userinfo aussi\n",
    "df_filtered_userinfo = df_userinfo[df_userinfo.id.isin(filtered_user_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAkhmCM1aeBV"
   },
   "source": [
    "*   Associer à chaque utilisateur sa liste d'amis et placer le résultat dans un *dataframe*: *df_user_friends*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_friends = df_filtered_freindship.groupby('userid1').userid2.apply(list).reset_index(name='friends_list') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u7g9w5IaeE_"
   },
   "source": [
    "* Calculer la fréquence de chaque paire *(utilisateur, POI)* et placer le résultat dans un *dataframe* *df_frequencies*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joindre df_filtered_checkins avec df_locations en utilisant les champs place_id et id\n",
    "df_checkins_locations = pd.merge(df_filtered_checkins, df_locations,left_on=\"placeid\",right_on=\"id\",how=\"left\") \n",
    "df_checkins_locations = df_checkins_locations.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>placeid</th>\n",
       "      <th>id</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>checkins_count</th>\n",
       "      <th>users_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27506</td>\n",
       "      <td>332616</td>\n",
       "      <td>332616.0</td>\n",
       "      <td>-1.777557</td>\n",
       "      <td>52.411873</td>\n",
       "      <td>67.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27506</td>\n",
       "      <td>32080</td>\n",
       "      <td>32080.0</td>\n",
       "      <td>-0.108469</td>\n",
       "      <td>51.555021</td>\n",
       "      <td>929.0</td>\n",
       "      <td>580.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27506</td>\n",
       "      <td>1290090</td>\n",
       "      <td>1290090.0</td>\n",
       "      <td>-1.700092</td>\n",
       "      <td>52.189657</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27506</td>\n",
       "      <td>1120128</td>\n",
       "      <td>1120128.0</td>\n",
       "      <td>-0.145848</td>\n",
       "      <td>51.513196</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27506</td>\n",
       "      <td>445729</td>\n",
       "      <td>445729.0</td>\n",
       "      <td>-0.147148</td>\n",
       "      <td>51.513221</td>\n",
       "      <td>31.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  placeid         id       lng        lat  checkins_count  \\\n",
       "0   27506   332616   332616.0 -1.777557  52.411873            67.0   \n",
       "1   27506    32080    32080.0 -0.108469  51.555021           929.0   \n",
       "2   27506  1290090  1290090.0 -1.700092  52.189657             6.0   \n",
       "3   27506  1120128  1120128.0 -0.145848  51.513196             7.0   \n",
       "4   27506   445729   445729.0 -0.147148  51.513221            31.0   \n",
       "\n",
       "   users_count  \n",
       "0         50.0  \n",
       "1        580.0  \n",
       "2          5.0  \n",
       "3          6.0  \n",
       "4         28.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_checkins_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencies = df_checkins_locations.groupby(['userid', 'placeid'])[\"id\"].count().reset_index(name=\"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joindre df_frequencies avec df_locations en utilisant les champs place_id et id\n",
    "df_frequencies = pd.merge(df_frequencies, df_locations,left_on=\"placeid\",right_on=\"id\",how=\"left\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niPnBwMqaeIg"
   },
   "source": [
    "* Mettre à jour les fréquences de *df_frequencies* pour les ramener à l'intervalle [0, 10] en appliquant une normalisation comme suit:\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?export=view&id=1vXvex4I5K30pmlsf2LQJ50nwCxt8XTA2' width=30%></center>\n",
    "\n",
    "où $f_{min}$ et $f_{max}$ sont respectivement le nombre minimal et maximal de fréquences de visites d'un POI quelconque dans le jeu de données. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculer f min\n",
    "f_min = df_frequencies['frequency'].min()\n",
    "# calculer f max\n",
    "f_max = df_frequencies['frequency'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencies[\"ratings\"] = df_frequencies[\"frequency\"].apply(lambda x: 10*np.tanh(10*(x-f_min)/(f_max-f_min)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>placeid</th>\n",
       "      <th>frequency</th>\n",
       "      <th>id</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>checkins_count</th>\n",
       "      <th>users_count</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>8904</td>\n",
       "      <td>1</td>\n",
       "      <td>8904</td>\n",
       "      <td>-94.607499</td>\n",
       "      <td>39.052318</td>\n",
       "      <td>114</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>8947</td>\n",
       "      <td>2</td>\n",
       "      <td>8947</td>\n",
       "      <td>-122.029631</td>\n",
       "      <td>37.331880</td>\n",
       "      <td>3100</td>\n",
       "      <td>1186</td>\n",
       "      <td>2.186351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>9073</td>\n",
       "      <td>1</td>\n",
       "      <td>9073</td>\n",
       "      <td>-122.393725</td>\n",
       "      <td>37.795339</td>\n",
       "      <td>2654</td>\n",
       "      <td>1659</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>9186</td>\n",
       "      <td>1</td>\n",
       "      <td>9186</td>\n",
       "      <td>-77.036594</td>\n",
       "      <td>38.897638</td>\n",
       "      <td>1603</td>\n",
       "      <td>1383</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>9591</td>\n",
       "      <td>1</td>\n",
       "      <td>9591</td>\n",
       "      <td>-122.159772</td>\n",
       "      <td>37.447908</td>\n",
       "      <td>1418</td>\n",
       "      <td>525</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  placeid  frequency    id         lng        lat  checkins_count  \\\n",
       "0      15     8904          1  8904  -94.607499  39.052318             114   \n",
       "1      15     8947          2  8947 -122.029631  37.331880            3100   \n",
       "2      15     9073          1  9073 -122.393725  37.795339            2654   \n",
       "3      15     9186          1  9186  -77.036594  38.897638            1603   \n",
       "4      15     9591          1  9591 -122.159772  37.447908            1418   \n",
       "\n",
       "   users_count   ratings  \n",
       "0           21  0.000000  \n",
       "1         1186  2.186351  \n",
       "2         1659  0.000000  \n",
       "3         1383  0.000000  \n",
       "4          525  0.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frequencies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTkMe9tuaeL-"
   },
   "source": [
    "* Charger *df_frequencies* dans le framework *Suprise* en utilisant la fonction *load_from_df()*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0, 10))\n",
    "data = Dataset.load_from_df(df_frequencies[['userid', 'placeid', 'ratings']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkOr3KjSaeSX"
   },
   "source": [
    "* Utiliser la fonction *train_test_split()* pour diviser *df_frequencies* en un ensemble d'apprentissage (*training set*, 75\\% du jeu de données) et un ensemble de test (*test set*, 25\\% du jeu de données). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWzx_7igaeXa"
   },
   "source": [
    "*  Associer à chaque utilisateur sa liste de POI visités et placer le résultat dans un *dataframe* *df_user_POI*.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_POI = df_frequencies.groupby('userid').placeid.apply(list).reset_index(name='POI_list') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>POI_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>[8904, 8947, 9073, 9186, 9591, 10299, 14710, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>[10817, 34906, 39911, 43229, 44243, 59855, 628...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>[22370, 32398, 34157, 37303, 6563736, 7360178]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>[17579, 286512, 290022, 644423, 1541920]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81</td>\n",
       "      <td>[9220, 9221, 9222, 9226, 9246, 9247, 9248, 924...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid                                           POI_list\n",
       "0      15  [8904, 8947, 9073, 9186, 9591, 10299, 14710, 2...\n",
       "1      37  [10817, 34906, 39911, 43229, 44243, 59855, 628...\n",
       "2      45     [22370, 32398, 34157, 37303, 6563736, 7360178]\n",
       "3      53           [17579, 286512, 290022, 644423, 1541920]\n",
       "4      81  [9220, 9221, 9222, 9226, 9246, 9247, 9248, 924..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_POI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctLA38IhX_GK"
   },
   "source": [
    "# 3. Influence géographique\n",
    "\n",
    "Le *dataframe* *df_user_POI* associe chaque utilisateur $u$ à la liste $L_u$ des POIs qu'il a visité. \n",
    "\n",
    "\n",
    "*   Utiliser *df_user_POI* pour calculer pour chaque utilisateur $u$ les distances $d_{ij}$ entre chaque paire de POIs visités: \n",
    "\n",
    "$\\forall p_i, p_j \\in L_u \\times L_u, d_{ij} = distance(p_i, p_j)$. \n",
    "\n",
    "On dénotera cette liste de distances par $D_u$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RCYoD9FXnwAT"
   },
   "outputs": [],
   "source": [
    "def distance(pi, pj):\n",
    "    \n",
    "    # recuperer latitute et longitude de premier place\n",
    "    lat0 = df_locations[df_locations.id == pi].lat.values\n",
    "    lng0 = df_locations[df_locations.id == pi].lng.values\n",
    "\n",
    "    # recuperer latitute et longitude de second place\n",
    "    lat1 = df_locations[df_locations.id == pj].lat.values\n",
    "    lng1 = df_locations[df_locations.id == pj].lng.values\n",
    "\n",
    "    # calculer distance in km en utilisant la librairie geopy\n",
    "\n",
    "    return geodesic((lat0,lng0), (lat1,lng1)).km\n",
    "\n",
    "def distance_pair_list(Lu):\n",
    "    # recuper toutes les combinaisons de paires de lieux deja visités\n",
    "    if len(Lu) != 1 :\n",
    "        pairs = list(combinations(Lu,2))\n",
    "    else:\n",
    "        pairs = []\n",
    "    \n",
    "    dist_list = []\n",
    "    \n",
    "    #calucler la distance entre chaque paire de place\n",
    "    if pairs != [] :\n",
    "        for pair in pairs :\n",
    "            dist_list.append(distance(pair[0], pair[1]))\n",
    "    return dist_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMuvQbc0aZfg"
   },
   "source": [
    "* Utiliser $D_u$ pour calculer la densité $f_u$ d'une distance quelconque comme suit:\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?export=view&id=1BXoT3vKbXQ6GllTIQkUIXB12xh72XiN5' width=30%/></center>\n",
    "\n",
    "$K(.)$ est choisi comme étant la densité d'une fonction gaussienne:\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?export=view&id=17QL_BH-mnu0IUAgJwGv-G7GUjNJF30Ac' width = 18%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPJJ-ZeDcbBA"
   },
   "source": [
    "Le paramètre de lissage $h = 1,06\\hat{\\sigma}n^{-1/5}$, où $n$ est le nombre de POIs présents dans $L_u$ et $\\hat{\\sigma}$ est l'écart type de $D_u$. Implémenter l'expression $\\hat{f}_u$ dans une fonction *density()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GC9aujQrng0W"
   },
   "outputs": [],
   "source": [
    "def k_gaussian(x):\n",
    "    return (1/np.sqrt(2*np.pi))*np.exp(-x**2/2)\n",
    "\n",
    "def density(Du, dij, h):\n",
    "    D2 = list(map(lambda x : (dij-x)/h,Du))\n",
    "    density = 1/(len(Du)*h)*sum(list(map(k_gaussian,D2)))\n",
    "    return density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUcipzKLc9bh"
   },
   "source": [
    "* La densité $\\hat{f}_u$ est utilisée pour estimer la probabilité qu'un POI $p_i$ non visité par un utilisateur $u$ corresponde aux préférences géographiques de $u$ étant donné son historique de visite. Afin d'obtenir cette probabilité, on calcule la distance entre $p_i$ et chacun des POIs de la liste $L_u$ et on estime ensuite la probabilité de chacune de ces distances en passant par $\\hat{f}_u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG2EtVPHdQpp"
   },
   "source": [
    "* La probabilité finale qu'un utilisateur $u$ visite un POI $p_i$ est obtenue comme suit:\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?export=view&id=1jq6OspCkQegNTWm6yDb8VEQFtrLOpDr_' width=30%/> </center>\n",
    "\n",
    "**P**(p_i \\| L_u) représente la probabilité que l'utilisateur $u$ visite le POI $p_i$ étant donné le critère géographique. Implémenter l'équation ci-dessus dans une fonction *geo_proba()* qui prend en entrée la liste $L_u$ et un POI, et qui retourne la probabilité de visite de ce POI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "68uYwtgToV2D"
   },
   "outputs": [],
   "source": [
    "def geo_proba(Lu, pi):\n",
    "\n",
    "    d = []\n",
    "    \n",
    "    # recuperer la longitude et la latitude de lieu candidat à recommender\n",
    "    lat_i =   df_locations[df_locations.id == pi].lat.values[0]\n",
    "    long_i =  df_locations[df_locations.id == pi].lng.values[0]\n",
    "    \n",
    "    # calculer toutes les distances entre le lieu candidat à recommender et chaque lieu déja visité dans liste Lu\n",
    "    for l in Lu :\n",
    "        long_j = df_locations[df_locations.id == l].lng.values[0]\n",
    "        lat_j = df_locations[df_locations.id == l].lat.values[0]\n",
    "        d.append(geodesic((lat_j,long_j), (lat_i,long_i)).km)\n",
    "    \n",
    "    # recuperer la distances entre chaque paire des lieux deja visité\n",
    "    Du = distance_pair_list(Lu)\n",
    "    \n",
    "    # calculer h\n",
    "    n = len(Lu)\n",
    "    sigma = np.std(Du)\n",
    "    h = 1.06*sigma*n**(-1/5)\n",
    "    \n",
    "    # calucler la densité\n",
    "    density_list = list(map(lambda x : density(Du, x, h),d))\n",
    "\n",
    "    return np.mean(density_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIMqXOBIdjGI"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ua6RCpqeAuU"
   },
   "source": [
    "# 4. Influence sociale\n",
    "\n",
    "Le *dataframe* *df_user_friends* associe chaque utilisateur $u$ à ses amis $F(u)$.\n",
    "\n",
    "*   Pour chaque paire d'utilisateurs $(u, v)$, on calcule leur *similarité sociale* en utilisant le coefficient de Jaccard comme suit:\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?export=view&id=1hwScp-pmMvNgNxduEdSUkIs4f7U9GEvE' width=30%></center>\n",
    "\n",
    "Implémenter ce coefficient dans la fonction *social_similarity()*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "YFIYH0upodrz"
   },
   "outputs": [],
   "source": [
    "def social_similarity(u, v):\n",
    "    # amis de u\n",
    "    list1 = df_user_friends[df_user_friends.userid1 == u].friends_list.values[0]\n",
    "    # amis de v\n",
    "    list2 = df_user_friends[df_user_friends.userid1 == v].friends_list.values[0]\n",
    "    sim = len(list(set(list1) & set(list2))) / len(list(set(list1) | set(list2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnOqZdzre-Yf"
   },
   "source": [
    "* Ce coefficient peut être utilisé dans un modèle de filtrage collaboratif comme suit:\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?export=view&id=1wGcIMVrXYiBOeOr7W35hrJdrisErcBxd' width=30%></center>\n",
    "\n",
    "$r_{ui}$ indique la fréquence de visite de $u$ dans $i$, extraite de *df_frequencies*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rate(u,i,trainset) : \n",
    "        \n",
    "    inner_iid = trainset.to_inner_iid(i) # get the inner id of the location\n",
    "    inner_uid = trainset.to_inner_uid(u) # get the inner id of the user\n",
    "\n",
    "    res_ir = trainset.ir[inner_iid] # get rates given for the location\n",
    "    uid_ir = list(map(lambda x :x[0],res_ir)) \n",
    "    rate_ir = list(map(lambda x :x[1],res_ir)) \n",
    "\n",
    "    if uid_ir.count(inner_uid) == 1 :\n",
    "        r = rate_ir[uid_ir.index(inner_uid)]\n",
    "    \n",
    "    # si le lieu n'a pas été noté par u\n",
    "    else : \n",
    "        r = 0\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_hat(user_i,j,trainset,F_i) :\n",
    "\n",
    "    rate_j = np.zeros(len(F_i))\n",
    "    sim_list = np.zeros(len(F_i))\n",
    "\n",
    "    for i,user in enumerate(F_i) :\n",
    "\n",
    "        rate_j[i] = get_rate(user,j,trainset)\n",
    "        sim_list[i] = social_similarity(user_i,user)\n",
    "\n",
    "    return np.dot(sim_list,rate_j) / max(np.sum(sim_list),0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwXxz_elfU7G"
   },
   "source": [
    "* La prédiction $\\hat{r}_{ui}$ peut être tranformée en probabilité comme suit:\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?export=view&id=1v1b0LZrsAdT8bBi-zrJjRjUnhAiBF-SY' width=30%></center>\n",
    "\n",
    "où $L$ est la liste de POIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_is_in_trainset(u) :\n",
    "\n",
    "    try :\n",
    "        trainset.to_inner_uid(u)\n",
    "        res = 1\n",
    "    except :\n",
    "        res = 0\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_is_in_trainset(i) :\n",
    "\n",
    "    try :\n",
    "        trainset.to_inner_iid(i)\n",
    "        res = 1\n",
    "    except :\n",
    "        res = 0\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "IoPKIuyWokiX"
   },
   "outputs": [],
   "source": [
    "def social_proba(u, i, Lu):\n",
    "    #reuperer la list de tous les lieux\n",
    "    L = list(np.unique(df_locations.id))\n",
    "    #recuperer la liste des lieux visités par u\n",
    "    # recuperer la liste des lieux qui n'ont pas été encore visités par user u\n",
    "    L_diff = list(set(L)- set(Lu))\n",
    "    # considerer seulement les lieux qui sont dans le training set\n",
    "    L_diff = [i for i in L_diff if item_is_in_trainset(i)]\n",
    "    # reduire la liste de lieux à recommender à 50 pour raison de complexité de calcul\n",
    "    L_diff = L_diff[:200]\n",
    "    # recuperer la liste des amis de u\n",
    "    F_i = df_user_friends[df_user_friends.userid1 == u].friends_list.values[0]\n",
    "    # consider seulement la liste des amis qui sont dans le training set\n",
    "    print()\n",
    "    F_i = [i for i in F_i if user_is_in_trainset(i)]\n",
    "\n",
    "    if F_i == [] :\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    else :\n",
    "\n",
    "        numerator = r_hat(u,i,trainset,F_i)\n",
    "        denominator = max(max(list(map(lambda x : r_hat(u,x,trainset,F_i),L_diff))),0.01)\n",
    "        \n",
    "        return numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8G8Gd1jZgHUk"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmi7Y9CngIH9"
   },
   "source": [
    "# 5. Génération et évaluation des recommandations\n",
    "\n",
    "* Afin de générer les recommandations, on calcule les scores de pertinence $\\hat{s}_{ui}$ pour un utilisateur $u$ et un POI $i$ comme suit:\n",
    "\n",
    "<center> <img src='https://drive.google.com/uc?export=view&id=1Z2GsqBKynyHXE0or99SB2PK3dRTsyIsg' width=20%> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnn5lTgygqgH"
   },
   "source": [
    "* L'équation ci-dessus doit être implémenter dans une nouvelle classe de recommandation. (https://surprise.readthedocs.io/en/stable/building_custom_algo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GvlBLz4hl0tM"
   },
   "outputs": [],
   "source": [
    "class GSLR(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "  \n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        self.trainset = trainset\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i): \n",
    "        \n",
    "        # Liste des lieux visité par utilisateur u\n",
    "        Lu = list(set(df_user_POI[df_user_POI.userid == u].POI_list.values[0]))\n",
    "        \n",
    "        \n",
    "        if not user_is_in_trainset(u) or not item_is_in_trainset(i):\n",
    "            return np.nan\n",
    "\n",
    "        else :       \n",
    "            if np.isnan(social_proba(u, i, Lu)):\n",
    "                score = geo_proba(Lu, i)\n",
    "            else :\n",
    "                score = (geo_proba(Lu, i) + social_proba(u, i, Lu)) / 2\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisateur  121599  et/ou lieu  420048  non existants dans la base.\n",
      "Utilisateur  261561  et/ou lieu  17646  non existants dans la base.\n",
      "Utilisateur  380243  et/ou lieu  619514  non existants dans la base.\n",
      "\n",
      "Probability of user  2634921  visits  6758198  is  0.02567067743028482\n",
      "Utilisateur  391630  et/ou lieu  554889  non existants dans la base.\n",
      "Utilisateur  351034  et/ou lieu  2962070  non existants dans la base.\n",
      "\n",
      "Probability of user  2163956  visits  6600101  is  0.0007908749364762526\n",
      "\n",
      "Probability of user  828748  visits  45307  is  0.05811903112962183\n",
      "Utilisateur  70204  et/ou lieu  169342  non existants dans la base.\n",
      "\n",
      "Probability of user  2207714  visits  335738  is  0.0005633826511897169\n"
     ]
    }
   ],
   "source": [
    "gslr = GSLR()\n",
    "gslr.fit(trainset)\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(testset[:10])):\n",
    "    prediction = gslr.estimate(testset[i][0], testset[i][1])\n",
    "    if np.isnan(prediction):\n",
    "        print('Utilisateur ', testset[i][0], \" et/ou lieu \", testset[i][1], \" non existants dans la base.\")\n",
    "    else:\n",
    "        print(\"Probability of user \", testset[i][0], \" visits \", testset[i][1], \" is \", prediction)\n",
    "    predictions.append(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNtf3Ww-g3-V"
   },
   "source": [
    "* Générer les recommandations pour chaque utilisateur inclus dans l'ensemble de test et calculer la RMSE, la précision (Precision@N) et le recall (Recall@N) de l'approche (plus de détails concernant la validation croisée ici: https://surprise.readthedocs.io/en/stable/model_selection.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozmIPZLlhAKD"
   },
   "source": [
    "* Comparer la performance de ce modèle avec les modèles standards implémentés dans *Surprise*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for algorithm in [SVD(), NMF()]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=2, verbose=False)\n",
    "    \n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)\n",
    "    \n",
    "pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO3ESMy9v84UQOmr2dpnnhI",
   "collapsed_sections": [],
   "name": "2_TP_POI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
